{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9026ec1d-d573-4f03-a54a-1fcb24c3f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# TensorFlow/Keras (Keras layers and optimizers below)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Custom\n",
    "import sys\n",
    "sys.path.insert(0, '../src/utils')\n",
    "from trace_handler import TraceHandler\n",
    "import constants\n",
    "from single_byte_evaluator import SingleByteEvaluator\n",
    "sys.path.insert(0, '../src/modeling')\n",
    "from network import Network\n",
    "\n",
    "# Suppress TensorFlow messages\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # 1 for INFO, 2 for INFO & WARNINGs, 3 for INFO & WARNINGs & ERRORs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4653d645-484f-4272-8b8e-562bf8c42ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling traces: 100%|██████████| 50000/50000 [00:21<00:00, 2309.23it/s]\n",
      "Labeling traces: 100%|██████████| 50000/50000 [00:21<00:00, 2308.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_th = TraceHandler('/prj/side_channel/PinataTraces/CURR/D1-K1_50k_500MHz + Resampled at 168MHz.trs')    \n",
    "    \n",
    "BYTE_IDX = 0\n",
    "N_CLASSES = 256\n",
    "\n",
    "x_train_tot = train_th.get_traces()\n",
    "y_train_tot = train_th.get_specific_labels(BYTE_IDX)\n",
    "y_train_tot = to_categorical(y_train_tot, N_CLASSES)\n",
    "\n",
    "# Test\n",
    "test_th = TraceHandler('/prj/side_channel/PinataTraces/CURR/D1-K2_50k_500MHz + Resampled at 168MHz.trs')\n",
    "\n",
    "x_test, y_test = test_th.generate_test(BYTE_IDX) \n",
    "y_test = to_categorical(y_test, N_CLASSES)\n",
    "\n",
    "test_plaintexts = test_th.get_plaintexts()\n",
    "true_key_byte = test_th.get_key()[BYTE_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be13de2-7a22-4ba4-8f64-6e0995364590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "HP_CHOICES = {#'kernel_initializer': ['random_normal', 'random_uniform', \n",
    "              #                       'truncated_normal', \n",
    "              #                       'zeros', 'ones', \n",
    "              #                       'glorot_normal', 'glorot_uniform',\n",
    "              #                       'he_normal', 'he_uniform',\n",
    "              #                       'identity', 'orthogonal', 'constant', 'variance_scaling'],\n",
    "              'kernel_initializer': ['random_normal', 'random_uniform', 'he_normal', 'he_uniform'],  \n",
    "              'activation':         ['relu', 'tanh'],\n",
    "              'hidden_layers':      [3, 4, 5],\n",
    "              'hidden_neurons':     [200, 300, 400, 500],\n",
    "              'dropout_rate':       [0.0, 0.2, 0.4],\n",
    "              'optimizer':          [SGD, Adam, RMSprop],\n",
    "              'learning_rate':      [1e-3, 1e-4, 1e-5],\n",
    "              'batch_size':         [100, 200, 500, 1000]}    \n",
    "\n",
    "def build_model(kernel_initializer, \n",
    "                activation, \n",
    "                hidden_layers, \n",
    "                hidden_neurons, \n",
    "                dropout_rate, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input\n",
    "    model.add(Dense(constants.TRACE_LEN,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    activation=activation))\n",
    "\n",
    "    # First BatchNorm\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Hidden\n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(Dense(hidden_neurons,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        activation=activation))\n",
    "\n",
    "        # Dropout\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Second BatchNorm\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output\n",
    "    model.add(Dense(256, activation='softmax')) ########################### 256 to be changed if the target is changed (HW, ...)\n",
    "\n",
    "    # Compilation\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e0013e-bc61-4810-9968-585fa1add0a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 1s 2ms/step - loss: 5.6327 - accuracy: 0.0083\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 5.4258 - accuracy: 0.0122\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 5.3307 - accuracy: 0.0154\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 5.6811 - accuracy: 0.0088\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 5.5103 - accuracy: 0.0100\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 5.3895 - accuracy: 0.0140\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 5.6640 - accuracy: 0.0094\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 5.5787 - accuracy: 0.0102\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 5.4434 - accuracy: 0.0147\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 5.6930 - accuracy: 0.0075\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 5.4352 - accuracy: 0.0106\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 5.3051 - accuracy: 0.0186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m estimator \u001b[38;5;241m=\u001b[39m KerasClassifier(build_model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator,\n\u001b[1;32m      7\u001b[0m                     \u001b[38;5;66;03m# n_jobs=-1,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m                     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m                     param_grid\u001b[38;5;241m=\u001b[39mHP_CHOICES)\n\u001b[0;32m---> 13\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:672\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    668\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcloned_parameters)\n\u001b[1;32m    670\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 672\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m    675\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/utils/metaestimators.py:288\u001b[0m, in \u001b[0;36m_safe_split\u001b[0;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[1;32m    286\u001b[0m         X_subset \u001b[38;5;241m=\u001b[39m X[np\u001b[38;5;241m.\u001b[39mix_(indices, train_indices)]\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     X_subset \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     y_subset \u001b[38;5;241m=\u001b[39m _safe_indexing(y, indices)\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/utils/__init__.py:378\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m~/MDM32/mdm32_env/lib/python3.8/site-packages/sklearn/utils/__init__.py:202\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    201\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = KerasClassifier(build_model, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator,\n",
    "                    # n_jobs=-1,\n",
    "                    verbose=1,\n",
    "                    return_train_score=True,\n",
    "                    cv=10,\n",
    "                    param_grid=HP_CHOICES)\n",
    "\n",
    "grid_result = grid.fit(x_train_tot, y_train_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d6ec9-b5a1-4e91-ad16-27e9963e6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc4c0a-027a-404c-a462-12b6cbd1760e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860d6c0-8c22-47a1-8bd8-f1483985901d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8d221-0545-471a-8c5d-98c04886ea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c07b413-41f8-425e-97b1-a3ab2c2b1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(EarlyStopping(monitor='val_loss', \n",
    "                               patience=5))\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',\n",
    "                                   factor=0.2,\n",
    "                                   patience=3,\n",
    "                                   min_lr=1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd89f54-5b8c-4f5b-a2e1-01bd6f25b883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "N_EXP = 10\n",
    "EPOCHS = 300\n",
    "\n",
    "for model in models:\n",
    "    net, net_hp = model\n",
    "    net = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21832401-1f74-40ba-9f5d-be21316a33a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Fold Crossvalidation Results: [(17, 0.903700003027916), (12, 0.5949400067329407), (6, 0.21301999874413013), (0, 0.20499999970197677), (11, 0.06648000022396446), (13, 0.06000000014901161), (2, 0.0575399998575449), (9, 0.05130000011995435), (7, 0.03784000016748905), (16, 0.034999999962747094), (3, 0.007779999962076544), (5, 0.006280000042170286), (15, 0.004679999966174364), (10, 0.00463999998755753), (19, 0.004419999965466559), (18, 0.004359999974258244), (4, 0.0041999999899417165), (14, 0.003920000023208558), (8, 0.0038200000301003454), (1, 0.003639999986626208)]\n"
     ]
    }
   ],
   "source": [
    "rev = True\n",
    "\n",
    "if METRIC == 'LOSS':\n",
    "    rev = False\n",
    "    \n",
    "results.sort(key=lambda x: x[1], reverse=rev)\n",
    "print()\n",
    "print(f'K-Fold Crossvalidation Results: {[(idx, loss) for idx, loss, _ in results]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2218405-a0a7-4fd8-a5ff-420424f60b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_net, best_net_hp = models[results[0][0]]\n",
    "best_net_histories = results[0][2]\n",
    "\n",
    "# f, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# tot_train_acc = np.array([history.history['accuracy'] for history in best_net_histories], dtype=object) # object because not all histories have same length (EarlyStopping)\n",
    "# avg_train_acc = np.mean(tot_train_acc, axis=0)\n",
    "\n",
    "# for history in best_net_histories:\n",
    "#     ax[0].plot(history.history['accuracy'], label='train_acc',color='r')\n",
    "#     ax[0].plot(history.history['val_accuracy'], label='val_acc',color='b')\n",
    "#     ax[0].set_xlabel('Epochs')\n",
    "#     ax[0].set_ylabel('Accuracy')\n",
    "#     ax[0].legend()\n",
    "#     ax[0].grid()\n",
    "    \n",
    "#     ax[1].plot(history.history['loss'], label='train_loss', color='r')\n",
    "#     ax[1].plot(history.history['val_loss'], label='val_loss', color='b')\n",
    "#     ax[1].set_xlabel('Epochs')\n",
    "#     ax[1].set_ylabel('Loss')\n",
    "#     ax[1].legend()\n",
    "#     ax[1].grid()\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5053b3a-d57f-49dc-a0d5-7110f9391ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel_initializer': 'he_normal',\n",
       " 'activation': 'relu',\n",
       " 'hidden_layers': 4,\n",
       " 'hidden_neurons': 600,\n",
       " 'dropout_rate': 0.0,\n",
       " 'optimizer': keras.optimizer_v2.adam.Adam,\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 1000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_net_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db455e09-e8d2-40c8-8e9e-d593070a021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ge_random_no_split(n_exp, hp, x_train_tot, y_train_tot, x_test, test_plaintexts, true_key_byte, byte_idx):\n",
    "    \n",
    "    network = build_model(hp)\n",
    "    network.fit(x_train_tot,\n",
    "                y_train_tot,\n",
    "                epochs=200,\n",
    "                batch_size=hp['batch_size'],\n",
    "                verbose=1)\n",
    "   \n",
    "    num_test_traces = len(x_test)\n",
    "    \n",
    "    tr_pltxt = list(zip(x_test, test_plaintexts))\n",
    "\n",
    "    ranks = []\n",
    "    for i in tqdm(range(n_exp)):\n",
    "        \n",
    "        random.shuffle(tr_pltxt)\n",
    "        curr_x_test = np.array([tr for tr, _ in tr_pltxt])\n",
    "        curr_test_platintexts = [pl for _, pl in tr_pltxt]\n",
    "        \n",
    "        preds = network.predict(curr_x_test)\n",
    "\n",
    "        evaluator = SingleByteEvaluator(test_plaintexts=curr_test_platintexts,\n",
    "                                        byte_idx=byte_idx,\n",
    "                                        label_preds=preds)\n",
    "\n",
    "        exp_ranks = []\n",
    "        for j in range(num_test_traces):\n",
    "            n_traces = j + 1\n",
    "            exp_ranks.append(evaluator.rank(true_key_byte, n_traces))\n",
    "\n",
    "        exp_ranks = np.array(exp_ranks)\n",
    "        ranks.append(exp_ranks)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    guessing_entropy = np.mean(ranks, axis=0)\n",
    "\n",
    "    return guessing_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5eba225-7f22-44f3-9587-8e0ec9b9715c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 1s 5ms/step - loss: 5.4323 - accuracy: 0.0139\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6583 - accuracy: 0.0413\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3358 - accuracy: 0.0652\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0729 - accuracy: 0.0883\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.8536 - accuracy: 0.1116\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.6287 - accuracy: 0.1440\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.4050 - accuracy: 0.1800\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1648 - accuracy: 0.2218\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8948 - accuracy: 0.2774\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.6277 - accuracy: 0.3358\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.3130 - accuracy: 0.4078\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.9733 - accuracy: 0.4944\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.6459 - accuracy: 0.5761\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.2872 - accuracy: 0.6745\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.0024 - accuracy: 0.7538\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.7108 - accuracy: 0.8394\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.9109\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.9573\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9846\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9957\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.9993\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.9057e-04 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.6950e-04 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.6162e-04 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.9455e-04 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.4076e-04 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.0023e-04 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.8338e-04 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.8046e-04 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.4282e-04 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.1145e-04 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.8714e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.6629e-04 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.3654e-04 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.0164e-04 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.8973e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.7343e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.6496e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3160e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.2664e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.0976e-04 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7075e-04 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6994e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6205e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.4723e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.1273e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.2023e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9959e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7506e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.6972e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7103e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5096e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.3455e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2580e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1520e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0188e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.9169e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8756e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.7937e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.6323e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.6917e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.4703e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.5181e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.4257e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.2875e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.2004e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.1277e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.1026e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.0096e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.9652e-04 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.8959e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.8658e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.7705e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.7517e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.6923e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.6009e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.5514e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.5893e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.4946e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.4581e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.4084e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.3407e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2654e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2414e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2373e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2342e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.2067e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.2122e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.1285e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.0788e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.0987e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.0054e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.9485e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.4550e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.2543e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.2701e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.0153e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.6574e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.7212e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.5061e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.6544e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.6299e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.3662e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.0506e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.9173e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.1781e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.5729e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.7147e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.3638e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.1969e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.7958e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.7368e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.5145e-05 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.2340e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3032e-05 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.1838e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.0747e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7437e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5534e-05 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.4321e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.2680e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.2203e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3168e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.2035e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9752e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7698e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5752e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5921e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5638e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.3532e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2332e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0903e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0477e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0848e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.9176e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8447e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8134e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.7764e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.5667e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.5232e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.4107e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.4442e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.2851e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.3301e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.2614e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.1911e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.9942e-05 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.0673e-05 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.9553e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.0196e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1.8972e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.8008e-05 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.7906e-05 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1.7771e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:34<00:00, 75.42s/it]\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 10000\n",
    "\n",
    "tmp_x_test = x_test[:TEST_SIZE]\n",
    "tmp_test_plaintexts = test_plaintexts[:TEST_SIZE]\n",
    "\n",
    "tmp_ge = ge_random_no_split(n_exp=10,\n",
    "                            hp=best_net_hp,\n",
    "                            x_train_tot=x_train_tot,\n",
    "                            y_train_tot=y_train_tot,\n",
    "                            x_test=tmp_x_test,\n",
    "                            test_plaintexts=tmp_test_plaintexts,\n",
    "                            true_key_byte=true_key_byte,\n",
    "                            byte_idx=BYTE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de06721-707b-489a-be0e-d7e2e793c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = 30\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(tmp_ge[:M], marker='o')\n",
    "ax.set_xlabel('Number of Traces')\n",
    "ax.set_ylabel('Guessing Entropy')\n",
    "ax.grid()\n",
    "_ = ax.set_xticks(range(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e691cdc3-a9e5-4f03-a6ed-48c94e598001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def ge_train(n_exp, hp, x_train_tot, y_train_tot, x_test_tot, test_plaintexts_tot, true_key_byte, byte_idx):\n",
    "    \n",
    "# #     num_test_traces = int(len(x_test_tot) / n_exp)\n",
    "\n",
    "# #     ranks = []\n",
    "# #     for i in tqdm(range(n_exp)):\n",
    "# #         start = i * num_test_traces\n",
    "# #         stop = start + num_test_traces\n",
    "        \n",
    "# #         x_test = x_test_tot[start:stop]\n",
    "# #         test_plaintexts = test_plaintexts_tot[start:stop]\n",
    "        \n",
    "# #         network = build_model(hp)\n",
    "# #         network.fit(x_train_tot,\n",
    "# #                     y_train_tot,\n",
    "# #                     epochs=EPOCHS,\n",
    "# #                     batch_size=hp['batch_size'],\n",
    "# #                     verbose=0)\n",
    "\n",
    "# #         preds = network.predict(x_test)\n",
    "\n",
    "# #         evaluator = SingleByteEvaluator(test_plaintexts=test_plaintexts,\n",
    "# #                                         byte_idx=byte_idx,\n",
    "# #                                         label_preds=preds)\n",
    "\n",
    "# #         exp_ranks = []\n",
    "# #         for j in range(num_test_traces):\n",
    "# #             n_traces = j + 1\n",
    "# #             exp_ranks.append(evaluator.rank(true_key_byte, n_traces))\n",
    "\n",
    "# #         exp_ranks = np.array(exp_ranks)\n",
    "# #         ranks.append(exp_ranks)\n",
    "\n",
    "# #     ranks = np.array(ranks)\n",
    "# #     guessing_entropy = np.mean(ranks, axis=0)\n",
    "\n",
    "# #     return guessing_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def ge(n_exp, network, hp, x_train_tot, y_train_tot, x_test_tot, test_plaintexts_tot, true_key_byte, byte_idx, train): \n",
    "    \n",
    "#     if not train:\n",
    "#         network.fit(x_train_tot,\n",
    "#                     y_train_tot,\n",
    "#                     epochs=200,\n",
    "#                     batch_size=hp['batch_size'],\n",
    "#                     verbose=1)\n",
    "   \n",
    "#     num_test_traces = int(len(x_test_tot) / n_exp)\n",
    "\n",
    "#     ranks = []\n",
    "#     for i in tqdm(range(n_exp)):\n",
    "#         start = i * num_test_traces\n",
    "#         stop = start + num_test_traces\n",
    "        \n",
    "#         x_test = x_test_tot[start:stop]\n",
    "#         test_plaintexts = test_plaintexts_tot[start:stop]\n",
    "        \n",
    "#         if train:\n",
    "#             network = build_model(hp)\n",
    "#             network.fit(x_train_tot,\n",
    "#                         y_train_tot,\n",
    "#                         epochs=200,\n",
    "#                         batch_size=hp['batch_size'],\n",
    "#                         verbose=1)\n",
    "        \n",
    "#         preds = network.predict(x_test)\n",
    "        \n",
    "#         preds_plaintexts = list(zip(preds, test_plaintexts))\n",
    "#         random.shuffle(preds_plaintexts)\n",
    "#         shuffled_preds = np.array([pred for pred, _ in preds_plaintexts])\n",
    "#         shuffled_plaintexts = np.array([pltxt for _, pltxt in preds_plaintexts])\n",
    "\n",
    "#         evaluator = SingleByteEvaluator(test_plaintexts=shuffled_plaintexts,\n",
    "#                                         byte_idx=byte_idx,\n",
    "#                                         label_preds=shuffled_preds)\n",
    "\n",
    "#         exp_ranks = []\n",
    "#         for j in range(num_test_traces):\n",
    "#             n_traces = j + 1\n",
    "#             exp_ranks.append(evaluator.rank(true_key_byte, n_traces))\n",
    "\n",
    "#         exp_ranks = np.array(exp_ranks)\n",
    "#         ranks.append(exp_ranks)\n",
    "\n",
    "#     ranks = np.array(ranks)\n",
    "#     guessing_entropy = np.mean(ranks, axis=0)\n",
    "\n",
    "#     return guessing_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c65daa4-5d0c-49bc-8799-77eed20a01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ge_only_test = ge(n_exp=10,\n",
    "#                   network=best_net,\n",
    "#                   hp=best_net_hp,\n",
    "#                   x_train_tot=x_train_tot,\n",
    "#                   y_train_tot=y_train_tot,\n",
    "#                   x_test_tot=x_test,\n",
    "#                   test_plaintexts_tot=test_plaintexts,\n",
    "#                   true_key_byte=true_key_byte,\n",
    "#                   byte_idx=BYTE_IDX,\n",
    "#                   train=False)\n",
    "\n",
    "# ge_train = ge(n_exp=10,\n",
    "#               network=best_net,\n",
    "#               hp=best_net_hp,\n",
    "#               x_train_tot=x_train_tot,\n",
    "#               y_train_tot=y_train_tot,\n",
    "#               x_test_tot=x_test,\n",
    "#               test_plaintexts_tot=test_plaintexts,\n",
    "#               true_key_byte=true_key_byte,\n",
    "#               byte_idx=BYTE_IDX,\n",
    "#               train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894f1b57-af24-4e39-a183-1f8b84750c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 20\n",
    "# f, ax = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# ax[0].plot(ge_only_test[:N], marker='o', color='r')\n",
    "# ax[0].set_title('Different tests')\n",
    "# ax[0].set_xlabel('Number of Traces')\n",
    "# ax[0].set_ylabel('Guessing Entropy')\n",
    "# ax[0].grid()\n",
    "# _ = ax[0].set_xticks(range(N))\n",
    "\n",
    "# ax[1].plot(ge_train[:N], marker='o', color='b')\n",
    "# ax[1].set_title('Different trains & tests')\n",
    "# ax[1].set_xlabel('Number of Traces')\n",
    "# ax[1].set_ylabel('Guessing Entropy')\n",
    "# ax[1].grid()\n",
    "# _ = ax[1].set_xticks(range(N))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93896d3-c5b2-445b-b78e-a454ddb75191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ge_NEW(n_exp, hp, x_train_tot, y_train_tot, x_test_tot, test_plaintexts_tot, true_key_byte, byte_idx):\n",
    "    \n",
    "#     network = build_model(hp)\n",
    "#     network.fit(x_train_tot,\n",
    "#                 y_train_tot,\n",
    "#                 epochs=200,\n",
    "#                 batch_size=hp['batch_size'],\n",
    "#                 verbose=1)\n",
    "   \n",
    "#     num_test_traces = int(len(x_test_tot) / n_exp)\n",
    "\n",
    "#     ranks = []\n",
    "#     for i in tqdm(range(n_exp)):\n",
    "#         start = i * num_test_traces\n",
    "#         stop = start + num_test_traces\n",
    "        \n",
    "#         x_test = x_test_tot[start:stop]\n",
    "#         test_plaintexts = test_plaintexts_tot[start:stop]\n",
    "        \n",
    "#         preds = network.predict(x_test)\n",
    "        \n",
    "#         preds_plaintexts = list(zip(preds, test_plaintexts))\n",
    "#         random.shuffle(preds_plaintexts)\n",
    "#         shuffled_preds = np.array([pred for pred, _ in preds_plaintexts])\n",
    "#         shuffled_plaintexts = np.array([pltxt for _, pltxt in preds_plaintexts])\n",
    "\n",
    "#         evaluator = SingleByteEvaluator(test_plaintexts=shuffled_plaintexts,\n",
    "#                                         byte_idx=byte_idx,\n",
    "#                                         label_preds=shuffled_preds)\n",
    "\n",
    "#         exp_ranks = []\n",
    "#         for j in range(num_test_traces):\n",
    "#             n_traces = j + 1\n",
    "#             exp_ranks.append(evaluator.rank(true_key_byte, n_traces))\n",
    "\n",
    "#         exp_ranks = np.array(exp_ranks)\n",
    "#         ranks.append(exp_ranks)\n",
    "\n",
    "#     ranks = np.array(ranks)\n",
    "#     guessing_entropy = np.mean(ranks, axis=0)\n",
    "\n",
    "#     return guessing_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81837d03-d570-41c7-8169-ee0a2ed8a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ge = ge_NEW(n_exp=10,\n",
    "#                 hp=best_net_hp,\n",
    "#                 x_train_tot=x_train_tot,\n",
    "#                 y_train_tot=y_train_tot,\n",
    "#                 x_test_tot=x_test,\n",
    "#                 test_plaintexts_tot=test_plaintexts,\n",
    "#                 true_key_byte=true_key_byte,\n",
    "#                 byte_idx=BYTE_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda71280-bd21-4e38-bf2a-1772377cb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 30\n",
    "\n",
    "# f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# ax.plot(new_ge[:M], marker='o')\n",
    "# ax.set_xlabel('Number of Traces')\n",
    "# ax.set_ylabel('Guessing Entropy')\n",
    "# ax.grid()\n",
    "# _ = ax.set_xticks(range(M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm32_env",
   "language": "python",
   "name": "mdm32_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
